{
  "models": [
    {
      "name": "qwen2.5:0.5b",
      "size": "1.5 GB",
      "hardware": "CPU friendly; 8GB RAM OK",
      "note": "Default fast option for low-power CPUs."
    },
    {
      "name": "tinyllama:1.1b",
      "size": "0.7 GB",
      "hardware": "Very CPU friendly; 4-8GB RAM",
      "note": "Tiny Llama baseline for fast CPU inference."
    },
    {
      "name": "smollm:135m",
      "size": "0.2 GB",
      "hardware": "Ultra-light CPU",
      "note": "Extremely small model for quick tests."
    },
    {
      "name": "smollm:360m",
      "size": "0.4 GB",
      "hardware": "Ultra-light CPU",
      "note": "Small model for fast responses on weak CPUs."
    },
    {
      "name": "phi3:mini",
      "size": "2.4 GB",
      "hardware": "CPU friendly; 8-12GB RAM",
      "note": "Microsoft Phi-3 mini, strong for its size."
    },
    {
      "name": "phi3:mini-4k",
      "size": "2.4 GB",
      "hardware": "CPU friendly; 8-12GB RAM",
      "note": "Phi-3 mini with short context; same size, faster prompt handling."
    },
    {
      "name": "qwen2.5:1.8b",
      "size": "4.3 GB",
      "hardware": "Better with fast CPU or light GPU",
      "note": "Higher quality than 0.5B while staying manageable."
    },
    {
      "name": "llama3.2:1b",
      "size": "2.0 GB",
      "hardware": "CPU friendly; 8-12GB RAM",
      "note": "Small generalist Llama3.2 build."
    },
    {
      "name": "llama3.1:8b",
      "size": "5.5 GB",
      "hardware": "Prefer discrete GPU or strong CPU; 16GB+ RAM",
      "note": "Better reasoning; slower on CPU-only."
    },
    {
      "name": "deepseek-r1:1.5b",
      "size": "1.1 GB",
      "hardware": "Very CPU friendly",
      "note": "DeepSeek's distilled fast reasoning model."
    },
    {
      "name": "deepseek-r1:7b",
      "size": "4.7 GB",
      "hardware": "Fast CPU or GPU",
      "note": "Balanced reasoning model."
    },
    {
      "name": "deepseek-r1:8b",
      "size": "4.9 GB",
      "hardware": "Fast CPU or GPU",
      "note": "Llama-3-distilled reasoning model."
    },
    {
      "name": "deepseek-coder:6.7b",
      "size": "3.8 GB",
      "hardware": "Fast CPU or GPU",
      "note": "Specialized for coding tasks."
    },
    {
      "name": "phi4",
      "size": "9.1 GB",
      "hardware": "16GB+ RAM or GPU",
      "note": "Microsoft's strong 14B reasoning model."
    },
    {
      "name": "gemma2:2b",
      "size": "1.6 GB",
      "hardware": "Very CPU friendly",
      "note": "Google's efficient edge model."
    },
    {
      "name": "gemma2:9b",
      "size": "5.4 GB",
      "hardware": "8GB+ RAM / GPU",
      "note": "High quality general purpose model."
    },
    {
      "name": "llama3.2:3b",
      "size": "2.0 GB",
      "hardware": "CPU friendly",
      "note": "Meta's edge-optimized text model."
    },
    {
      "name": "mistral-nemo",
      "size": "7.1 GB",
      "hardware": "12GB+ RAM or GPU",
      "note": "New 12B standard from Mistral/NVIDIA."
    },
    {
      "name": "neural-chat",
      "size": "4.1 GB",
      "hardware": "Standard CPU/GPU",
      "note": "Intel-optimized 7B model."
    },
    {
      "name": "mistral:7b",
      "size": "4.1 GB",
      "hardware": "Fast CPU or GPU recommended",
      "note": "Well-known 7B general model."
    }
  ]
}
