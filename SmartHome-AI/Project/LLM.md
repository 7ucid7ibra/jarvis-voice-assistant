We want to use a [[L-LLM]] in the long run. 
I still have to [[Research]] the local language models and their specifications to figure out which one will be the most fitting for our use case.

For now I will use a LLM in the cloud via API key for testing, 
since [[My Hardware]] is not as dependable for local models.